{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b98eaffda2f8dccdd434b8a538c0dff68c3263c07983948f9c516c57469e3c42",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets, ReadInstruction\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")"
   ]
  },
  {
   "source": [
    "## 1. Amazon Review"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['Kitchen_v1_00', 'Electronics_v1_00', 'Books_v1_01', 'Video_DVD_v1_00']"
   ]
  },
  {
   "source": [
    "## 2. IMDB-SST2 Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imdb_sst2_loaders(config, tokenizer):\n",
    "\n",
    "    \"\"\"\n",
    "        We have to ensure that sample size as well label distribution remains uniform across the distribution and set. \n",
    "    \"\"\"\n",
    "\n",
    "    sst2 = load_dataset(\"toriving/sst2\") # sst2 has train, valid and test. We're mering test and valid set into test set\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "\n",
    "    sst2_train = sst2['train'].shuffle()\n",
    "    sst2_test = concatenate_datasets([sst2['validation'], sst2['test']]).shuffle()\n",
    "\n",
    "    imdb_train = imdb['train'].shuffle()\n",
    "    imdb_test = imdb['test'].shuffle()\n",
    "\n",
    "\n",
    "    train_label_values = []\n",
    "    test_label_values = []\n",
    "\n",
    "    labels = np.unique(sst2_train['label']).tolist()\n",
    "\n",
    "    for label in labels: # assuming that there's no label shift \n",
    "\n",
    "        # min number of samples of label in  both dataset  in train dataset\n",
    "        train_min = min(len(sst2_train.filter(lambda example: example['label'] == int(label))), len(imdb_train.filter(lambda example: example['label'] == int(label))))\n",
    "        \n",
    "        train_label_values.append(train_min)\n",
    "\n",
    "        # min number of samples of label in  both dataset  in test dataset\n",
    "        test_min = min(len(sst2_test.filter(lambda example: example['label'] == int(label))), len(imdb_test.filter(lambda example: example['label'] == int(label))))\n",
    "        test_label_values.append(test_min)\n",
    "    \n",
    "\n",
    "    train_label_dist = min(train_label_values)\n",
    "    \n",
    "    test_label_dist = min(test_label_values)\n",
    "\n",
    "\n",
    "    ## \n",
    "    dsets = {\n",
    "\n",
    "        \"sst2\":{\n",
    "            \"train\":[],\n",
    "            \"test\":[]\n",
    "        },\n",
    "\n",
    "        \"imdb\":{\n",
    "            \"train\":[],\n",
    "            \"test\":[]\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "        sst2_train_label = sst2_train.shuffle().filter(lambda example: example['label']==int(label)).select(range(train_label_dist))\n",
    "\n",
    "        sst2_test_label = sst2_test.shuffle().filter(lambda example: example['label']==int(label)).select(range(test_label_dist))\n",
    "\n",
    "        imdb_train_label = imdb_train.shuffle().filter(lambda example: example['label']==int(label)).select(range(train_label_dist))\n",
    "\n",
    "        imdb_test_label = imdb_test.shuffle().filter(lambda example: example['label']==int(label)).select(range(test_label_dist))\n",
    "\n",
    "\n",
    "        dsets['sst2']['train'].append(sst2_train_label)\n",
    "        dsets['sst2']['test'].append(sst2_test_label)\n",
    "\n",
    "        dsets['imdb']['train'].append(imdb_train_label)\n",
    "        dsets['imdb']['test'].append(imdb_test_label)\n",
    "        \n",
    "\n",
    "    ## split the data based on sample distribution as well as label distribution\n",
    "    sst2_train = concatenate_datasets(dsets=dsets['sst2']['train']).shuffle()\n",
    "    sst2_test = concatenate_datasets(dsets=dsets['sst2']['test']).shuffle()\n",
    "\n",
    "    imdb_train = concatenate_datasets(dsets=dsets['imdb']['train']).shuffle()\n",
    "    imdb_test = concatenate_datasets(dsets=dsets['imdb']['test']).shuffle()\n",
    "\n",
    "\n",
    "    # tokenize the dataset\n",
    "\n",
    "    # this can be done with loop but who cares \n",
    "\n",
    "    # sst2\n",
    "    # train\n",
    "    sst2_train_tokenized = sst2_train.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    sst2_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    # test\n",
    "    sst2_test_tokenized = sst2_test.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    sst2_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "\n",
    "    # imdb\n",
    "    # train\n",
    "    imdb_train_tokenized = imdb_train.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    imdb_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    # test\n",
    "    imdb_test_tokenized = imdb_test.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    imdb_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "\n",
    "    # print(sst2_train_tokenized, sst2_test_tokenized, imdb_train_tokenized, imdb_test_tokenized)\n",
    "\n",
    "    sst2_trainloader = torch.utils.data.DataLoader(dataset = sst2_train_tokenized, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "\n",
    "    sst2_testloader = torch.utils.data.DataLoader(dataset = sst2_test_tokenized, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "    imdb_trainloader = torch.utils.data.DataLoader(dataset = imdb_train_tokenized, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "\n",
    "    imdb_testloader = torch.utils.data.DataLoader(dataset = imdb_test_tokenized, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "    return {\n",
    "        \"sst2\":{\n",
    "            \"train\":sst2_trainloader,\n",
    "            \"valid\":sst2_testloader,\n",
    "            \"test\":sst2_testloader,\n",
    "        },\n",
    "        \"imdb\":{\n",
    "            \"train\":imdb_trainloader,\n",
    "            \"valid\":imdb_testloader,\n",
    "            \"test\":imdb_testloader\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ss_t2 (/home/macab/.cache/huggingface/datasets/ss_t2/default/0.0.0/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa)\n",
      "Reusing dataset imdb (/home/macab/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "100%|██████████| 7/7 [00:00<00:00, 35.05ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.82ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 58.81ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 31.54ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 34.59ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.49ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 59.12ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.61ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 35.17ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 37.41ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.67ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.24ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 34.56ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 36.28ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 31.37ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 30.55ba/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.61ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.69ba/s]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.22s/ba]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.04s/ba]\n"
     ]
    }
   ],
   "source": [
    "loaders = imdb_sst2_loaders(config=config['tasks']['imdb_sst2_sa'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sst2 828 335 335\nimdb 828 335 335\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders:\n",
    "    print(domain, len(loaders[domain]['train']), len(loaders[domain]['test']), len(loaders[domain]['valid']))"
   ]
  },
  {
   "source": [
    "## 3. MNLI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnli_loaders(config, tokenizer):\n",
    "\n",
    "    domains = config['domains']\n",
    "\n",
    "    train = 'train'\n",
    "    test = 'validation_matched'\n",
    "\n",
    "    # load the dataset\n",
    "    dataset = load_dataset(\"multi_nli\")\n",
    "    dataset = dataset.remove_columns(['pairID', 'promptID', 'premise_binary_parse', 'premise_parse', 'hypothesis_binary_parse', 'hypothesis_parse']) # remove the unrelated fields\n",
    "\n",
    "    labels = set(dataset['train']['label'])\n",
    "\n",
    "    # which label has least number of samples in train data as well as (valid)validation data in all domains\n",
    "    train_label_dist = 25000 # manually checked \n",
    "    test_label_dist = 600 # manually checked \n",
    " \n",
    "\n",
    "    domain_dsets = {}\n",
    "    for domain in domains:\n",
    "\n",
    "        domain_dsets[domain] = {\n",
    "            \"train\":[]\n",
    "        }\n",
    "        domain_dsets[domain].update({\"test\":[]})\n",
    "\n",
    "    \n",
    "    for label in labels:\n",
    "\n",
    "        for domain in domains:\n",
    "\n",
    "            train = dataset['train'].filter(lambda example:example['genre']==domain).filter(lambda example:example['label']==label).select(range(train_label_dist))\n",
    "            test = dataset['validation_matched'].filter(lambda example:example['genre']==domain).filter(lambda example:example['label']==label).select(range(test_label_dist))\n",
    "\n",
    "            domain_dsets[domain]['train'].append(train)\n",
    "            domain_dsets[domain]['test'].append(test)\n",
    "    \n",
    "\n",
    "    # concatenate the dataset and shuffle them\n",
    "    # before it would be list of datasets and after it will be single dataset\n",
    "    for domain in domains:\n",
    "\n",
    "        # concate class label datasets\n",
    "        domain_dsets[domain]['train'] = concatenate_datasets(dsets=domain_dsets[domain]['train']).shuffle()\n",
    "        domain_dsets[domain]['test'] = concatenate_datasets(dsets=domain_dsets[domain]['test']).shuffle()\n",
    "\n",
    "\n",
    "        # # tokenize \n",
    "        domain_dsets[domain]['train'] = domain_dsets[domain]['train'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        domain_dsets[domain]['test'] = domain_dsets[domain]['test'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # change the dtype \n",
    "        domain_dsets[domain]['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        domain_dsets[domain]['test'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "        # create dataloaders\n",
    "        domain_dsets[domain]['train'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['train'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['test'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['test'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['valid'] = domain_dsets[domain]['test'] # validation and test will be same\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # why the hell I am doing this?\n",
    "    loaders = domain_dsets\n",
    "    \n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0c1db9bb01906573.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-898cee5d2d382de5.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e49de0ed3ac548b0.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-153c9870a40f1da6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e592acff3004bab5.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-23008dd01a782d9c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5dedd4fb23f94a5f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0b183fbad431cd93.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-4d9005413613dfcc.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-50e3f6d87c2d2588.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b655ab8c4b69128f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b2a0e2a28d47007c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-f421b45446c59ac7.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a538299d4fd28437.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5c0024f9bf3852ee.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fd3ed81f369e9c03.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-927d76345e49252b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-cb296e615cf0c952.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-6fa6fae9dcab5274.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-1cd36db539da77d2.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2ce20719b0f395e0.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-6ba23de814dc90b4.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc45afa6ba2e9bdb.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-17f1d6f29172af7a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-79960e66c0ba2acd.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b3b8ba88c5419e90.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-4acd6f099c462a6b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-1b5919b2778fe34a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2f865b9b2e23546a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-aa67b9875687f5f3.arrow\n",
      "Loading cached shuffled indices for dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fe49e24b8c45062d.arrow\n",
      "100%|██████████| 75/75 [00:51<00:00,  1.46ba/s]\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fe27cbbacb98f6d4.arrow\n",
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.99ba/s]\n",
      "100%|██████████| 75/75 [00:33<00:00,  2.25ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.39ba/s]\n",
      "100%|██████████| 75/75 [00:48<00:00,  1.54ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.67ba/s]\n",
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.75ba/s]\n"
     ]
    }
   ],
   "source": [
    "loaders = mnli_loaders(config=config['tasks'][task], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n"
     ]
    }
   ],
   "source": [
    "data = dsets['slate']['train'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "government 2344 57 57\ntelephone 2344 57 57\nfiction 2344 57 57\ntravel 2344 57 57\nslate 2344 57 57\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders:\n",
    "    print(domain, len(loaders[domain]['train']), len(loaders[domain]['test']), len(loaders[domain]['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## 4. Paraphrase Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task=\"paraphrase\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_loaders(config, tokenizer, max_len=256):\n",
    "\n",
    "    domains = config['domains']\n",
    "\n",
    "    # which label has least number of samples in train data as well as (test)validation data in all domains\n",
    "    train_label_dist = 21829 # manually checked \n",
    "    test_label_dist = 7075 # manually checked \n",
    "\n",
    "\n",
    "    # we are  not going to take words greater than 256\n",
    "    paws = load_dataset(\"paws\", 'labeled_final')\n",
    "    qqp = load_dataset(\"glue\", 'qqp')\n",
    "\n",
    "\n",
    "    # # If you want to filter the data based on length | no filtering in actul experiment\n",
    "    for _, (paws_set, qqp_set) in enumerate(zip(paws.keys(), qqp.keys())):\n",
    "\n",
    "        # # applying filter\n",
    "        # paws[paws_set] = paws[paws_set].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=max_len)\n",
    "        # qqp[qqp_set] = qqp[qqp_set].filter(lambda example : (len(example['question1'])+len(example['question2']))<=max_len)\n",
    "\n",
    "        # both paws and qqp has difference names for 2 input sentences \n",
    "        # paws = (sentence1, sentence2) and qqp = (question1, question2) update qqp col to match paws\n",
    "\n",
    "        qqp[qqp_set] = qqp[qqp_set].rename_column('question1', 'sentence1') \n",
    "        qqp[qqp_set] = qqp[qqp_set].rename_column('question2', 'sentence2')\n",
    "\n",
    "\n",
    "    # # merge the validation and test of both datasets\n",
    "    paws['test'] = concatenate_datasets(dsets=[paws['test'], paws['validation']])\n",
    "    qqp['test'] = concatenate_datasets(dsets=[qqp['test'], qqp['validation']])\n",
    "\n",
    "    datasets = {\n",
    "        \"paws\":paws,\n",
    "        \"qqp\":qqp\n",
    "    }\n",
    "\n",
    "    labels = list(set(qqp['train']['label']))\n",
    "\n",
    "    domain_dsets = {}\n",
    "    for domain in domains:\n",
    "\n",
    "        domain_dsets[domain] = {\n",
    "            \"train\":[]\n",
    "        }\n",
    "        domain_dsets[domain].update({\"test\":[]})\n",
    "\n",
    "    # take equal numbe of samples for each domain for each label for each set\n",
    "    for label in labels:\n",
    "        \n",
    "        for domain in domain_dsets:\n",
    "\n",
    "            train = datasets[domain]['train'].filter(lambda example:example['label']==label).shuffle().select(range(train_label_dist))\n",
    "            test = datasets[domain]['test'].filter(lambda example:example['label']==label).shuffle().select(range(test_label_dist))\n",
    "\n",
    "            domain_dsets[domain]['train'].append(train)\n",
    "            domain_dsets[domain]['test'].append(test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # concatenate the dataset and shuffle them\n",
    "    # before it would be list of datasets and after it will be single dataset\n",
    "    for domain in domains:\n",
    "\n",
    "        # concate class label datasets\n",
    "        domain_dsets[domain]['train'] = concatenate_datasets(dsets=domain_dsets[domain]['train']).shuffle()\n",
    "        domain_dsets[domain]['test'] = concatenate_datasets(dsets=domain_dsets[domain]['test']).shuffle()\n",
    "\n",
    "\n",
    "        # # tokenize \n",
    "        domain_dsets[domain]['train'] = domain_dsets[domain]['train'].map(lambda x: tokenizer(x['sentence1'], x['sentence2'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        domain_dsets[domain]['test'] = domain_dsets[domain]['test'].map(lambda x: tokenizer(x['sentence1'], x['sentence2'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # change the dtype \n",
    "        domain_dsets[domain]['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        domain_dsets[domain]['test'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "        # create dataloaders\n",
    "        domain_dsets[domain]['train'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['train'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['test'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['test'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['valid'] = domain_dsets[domain]['test'] # validation and test will be same\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # why the hell I am doing this?\n",
    "    loaders = domain_dsets\n",
    "    \n",
    "\n",
    "    return loaders\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paws (/home/macab/.cache/huggingface/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34)\n",
      "Reusing dataset glue (/home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 364/364 [00:08<00:00, 43.55ba/s]\n",
      "100%|██████████| 432/432 [00:08<00:00, 49.26ba/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 48.48ba/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 41.63ba/s]\n",
      "100%|██████████| 364/364 [00:07<00:00, 47.04ba/s]\n",
      "100%|██████████| 432/432 [00:08<00:00, 49.18ba/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.91ba/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 48.24ba/s]\n",
      "100%|██████████| 44/44 [00:21<00:00,  2.04ba/s]\n",
      "100%|██████████| 15/15 [00:06<00:00,  2.19ba/s]\n",
      "100%|██████████| 44/44 [00:35<00:00,  1.23ba/s]\n",
      "100%|██████████| 15/15 [00:11<00:00,  1.28ba/s]\n"
     ]
    }
   ],
   "source": [
    "loaders = paraphrase_loaders(config=config['tasks'][task], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "qqp 1365 443 443\npaws 1365 443 443\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders:\n",
    "    print(domain, len(loaders[domain]['train']), len(loaders[domain]['test']), len(loaders[domain]['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paws (/home/macab/.cache/huggingface/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34)\n",
      "Reusing dataset glue (/home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "paws, qqp = paraphrase_loaders(config=config['tasks'][task], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(qqp['train']['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'qqp': {'train': [], 'test': []}, 'paws': {'train': [], 'test': []}}"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "domain_dsets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dsets = paraphrase_loaders(config=config['tasks'][task], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'qqp': {'train': [], 'test': []}, 'paws': {'train': [], 'test': []}}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "domain_dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1]), array([27572, 21829]))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "np.unique(paws['train']['label'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1]), array([8925, 7075]))"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "np.unique(concatenate_datasets(dsets=[paws['test'], paws['validation']])['label'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdsets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Converts a list of :class:`Dataset` with the same schema into a single :class:`Dataset`.\n",
      "\n",
      "Args:\n",
      "    dsets (:obj:`List[datasets.Dataset]`): List of Datasets to concatenate.\n",
      "    info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      "    split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      "    axis (``{0, 1}``, default ``0``, meaning over rows):\n",
      "        Axis to concatenate over, where ``0`` means over rows (vertically) and ``1`` means over columns\n",
      "        (horizontally).\n",
      "\n",
      "        .. versionadded:: 1.6.0\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ],
     "name": "stdout"
    }
   ],
   "source": [
    "concatenate_datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (/home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Reusing dataset paws (/home/macab/.cache/huggingface/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34)\n"
     ]
    }
   ],
   "source": [
    "qqp = load_dataset(\"glue\", 'qqp')\n",
    "paws = load_dataset(\"paws\", 'labeled_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 49401\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "paws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 139
    }
   ],
   "source": [
    "qqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 49401\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "paws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train train\ntest validation\nvalidation test\n"
     ]
    }
   ],
   "source": [
    "for _, (paws_set, qqp_set) in enumerate(zip(paws.keys(), qqp.keys())):\n",
    "    print(paws_set, qqp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 45.38ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 45.87ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.58ba/s]32676 5278 5418\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(paws['train'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=256)), len(paws['test'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=256)), len(paws['validation'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 364/364 [00:07<00:00, 46.61ba/s]\n",
      "100%|██████████| 391/391 [00:08<00:00, 45.49ba/s]\n",
      "100%|██████████| 41/41 [00:00<00:00, 46.21ba/s]243752 257088 26991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(qqp['train'].filter(lambda example : (len(example['question1'])+len(example['question2']))<=256)), len(qqp['test'].filter(lambda example : (len(example['question1'])+len(example['question2']))<=256)), len(qqp['validation'].filter(lambda example : (len(example['question1'])+len(example['question2']))<=256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'What is the step by step guide to invest in share market in india?'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "dataset['train']['questions'][0]['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paws (/home/macab/.cache/huggingface/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34)\n"
     ]
    }
   ],
   "source": [
    "paws = load_dataset(\"paws\", 'labeled_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train (array([0, 1]), array([27572, 21829]))\ntest (array([0, 1]), array([4464, 3536]))\nvalidation (array([0, 1]), array([4461, 3539]))\n"
     ]
    }
   ],
   "source": [
    "for set in paws.keys():\n",
    "    print(set, np.unique(paws[set]['label'], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'validation'])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset glue/qqp (download: 39.76 MiB, generated: 106.55 MiB, post-processed: Unknown size, total: 146.32 MiB) to /home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
      "Downloading: 100%|██████████| 41.7M/41.7M [00:06<00:00, 6.46MB/s]\n",
      "Dataset glue downloaded and prepared to /home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "qqp = load_dataset(\"glue\", 'qqp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (/home/macab/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "qqp = load_dataset(\"glue\", 'qqp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 364/364 [00:08<00:00, 44.60ba/s]\n"
     ]
    }
   ],
   "source": [
    "qqp['train'] = qqp['train'].filter(lambda example : (len(example['question1'])+len(example['question2']))<=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 243752\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "qqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "415afca1a6f6454e930c6fe9b3808b25"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "ls_1 = []\n",
    "ls_2 = []\n",
    "\n",
    "\n",
    "for _, (q1, q2) in tqdm(enumerate(zip(qqp['train']['question1'], qqp['train']['question2']))):\n",
    "\n",
    "    ls_1.append(len(q2))\n",
    "    ls_2.append(len(q2))\n",
    "    # print(q1, q2)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "243752 243752\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_1), len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(min(ls_1), min(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "122 122\n"
     ]
    }
   ],
   "source": [
    "print(max(ls_1), max(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "44.41006022514687 44.41006022514687\n"
     ]
    }
   ],
   "source": [
    "print(sum(ls_1)/len(ls_1), sum(ls_2)/len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset paws (/home/macab/.cache/huggingface/datasets/paws/labeled_final/1.1.0/09d8fae989bb569009a8f5b879ccf2924d3e5cd55bfe2e89e6dab1c0b50ecd34)\n"
     ]
    }
   ],
   "source": [
    "paws = load_dataset(\"paws\", 'labeled_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 43.78ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.21ba/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 45.03ba/s]\n"
     ]
    }
   ],
   "source": [
    "paws['train'] = paws['train'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=250)\n",
    "paws['test'] = paws['test'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=250)\n",
    "paws['validation'] = paws['validation'].filter(lambda example : (len(example['sentence1'])+len(example['sentence2']))<=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 31025\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 5024\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 5212\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "paws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "51a71f908dee47e085802be5aca24ba1"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "ls_1 = []\n",
    "ls_2 = []\n",
    "\n",
    "\n",
    "\n",
    "for _, (q1, q2) in tqdm(enumerate(zip(paws['train']['sentence1'], paws['train']['sentence2']))):\n",
    "\n",
    "    ls_1.append(len(q2))\n",
    "    ls_2.append(len(q2))\n",
    "    # print(q1, q2)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49401 49401\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_1), len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6 6\n"
     ]
    }
   ],
   "source": [
    "print(min(ls_1), min(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "225 225\n"
     ]
    }
   ],
   "source": [
    "print(max(ls_1), max(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "113.75154349102245 113.75154349102245\n"
     ]
    }
   ],
   "source": [
    "print(sum(ls_1)/len(ls_1), sum(ls_2)/len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 363846\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 40430\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question1', 'question2', 'label', 'idx'],\n",
       "        num_rows: 390965\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (/home/macab/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "mnli = load_dataset(\"glue\", 'mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 393/393 [00:08<00:00, 45.52ba/s]\n"
     ]
    }
   ],
   "source": [
    "mnli['train'] = mnli['train'].filter(lambda example : (len(example['premise'])+len(example['hypothesis']))<=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95afdc1e08d143e3a4c4b35a2155c1df"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "ls_1 = []\n",
    "ls_2 = []\n",
    "\n",
    "\n",
    "\n",
    "for _, (q1, q2) in tqdm(enumerate(zip(mnli['train']['premise'], mnli['train']['hypothesis']))):\n",
    "\n",
    "    ls_1.append(len(q2))\n",
    "    ls_2.append(len(q2))\n",
    "    # print(q1, q2)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "143489 143489\n"
     ]
    }
   ],
   "source": [
    "print(len(ls_1), len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "110 110\n"
     ]
    }
   ],
   "source": [
    "print(max(ls_1), max(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "38.63926154618124 38.63926154618124\n"
     ]
    }
   ],
   "source": [
    "print(sum(ls_1)/len(ls_1), sum(ls_2)/len(ls_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /home/macab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n",
      "Downloading: 6.22kB [00:00, 3.87MB/s]\n",
      "Downloading: 1.05MB [00:01, 670kB/s]\n",
      "Downloading: 441kB [00:00, 457kB/s]\n",
      "                                Dataset glue downloaded and prepared to /home/macab/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "mrpc = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "mrpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}