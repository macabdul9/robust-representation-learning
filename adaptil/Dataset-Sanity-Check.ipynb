{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0b98eaffda2f8dccdd434b8a538c0dff68c3263c07983948f9c516c57469e3c42",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets, ReadInstruction\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")"
   ]
  },
  {
   "source": [
    "## 1. Amazon Review"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = ['Kitchen_v1_00', 'Electronics_v1_00', 'Books_v1_01', 'Video_DVD_v1_00']"
   ]
  },
  {
   "source": [
    "## 2. IMDB-SST2 Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imdb_sst2_loaders(config, tokenizer):\n",
    "\n",
    "    \"\"\"\n",
    "        We have to ensure that sample size as well label distribution remains uniform across the distribution and set. \n",
    "    \"\"\"\n",
    "\n",
    "    sst2 = load_dataset(\"toriving/sst2\") # sst2 has train, valid and test. We're mering test and valid set into test set\n",
    "    imdb = load_dataset(\"imdb\")\n",
    "\n",
    "    sst2_train = sst2['train'].shuffle()\n",
    "    sst2_test = concatenate_datasets([sst2['validation'], sst2['test']]).shuffle()\n",
    "\n",
    "    imdb_train = imdb['train'].shuffle()\n",
    "    imdb_test = imdb['test'].shuffle()\n",
    "\n",
    "\n",
    "    train_label_values = []\n",
    "    test_label_values = []\n",
    "\n",
    "    labels = np.unique(sst2_train['label']).tolist()\n",
    "\n",
    "    for label in labels: # assuming that there's no label shift \n",
    "\n",
    "        # min number of samples of label in  both dataset  in train dataset\n",
    "        train_min = min(len(sst2_train.filter(lambda example: example['label'] == int(label))), len(imdb_train.filter(lambda example: example['label'] == int(label))))\n",
    "        \n",
    "        train_label_values.append(train_min)\n",
    "\n",
    "        # min number of samples of label in  both dataset  in test dataset\n",
    "        test_min = min(len(sst2_test.filter(lambda example: example['label'] == int(label))), len(imdb_test.filter(lambda example: example['label'] == int(label))))\n",
    "        test_label_values.append(test_min)\n",
    "    \n",
    "\n",
    "    train_label_dist = min(train_label_values)\n",
    "    \n",
    "    test_label_dist = min(test_label_values)\n",
    "\n",
    "\n",
    "    ## \n",
    "    dsets = {\n",
    "\n",
    "        \"sst2\":{\n",
    "            \"train\":[],\n",
    "            \"test\":[]\n",
    "        },\n",
    "\n",
    "        \"imdb\":{\n",
    "            \"train\":[],\n",
    "            \"test\":[]\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "    for label in labels:\n",
    "\n",
    "        sst2_train_label = sst2_train.shuffle().filter(lambda example: example['label']==int(label)).select(range(train_label_dist))\n",
    "\n",
    "        sst2_test_label = sst2_test.shuffle().filter(lambda example: example['label']==int(label)).select(range(test_label_dist))\n",
    "\n",
    "        imdb_train_label = imdb_train.shuffle().filter(lambda example: example['label']==int(label)).select(range(train_label_dist))\n",
    "\n",
    "        imdb_test_label = imdb_test.shuffle().filter(lambda example: example['label']==int(label)).select(range(test_label_dist))\n",
    "\n",
    "\n",
    "        dsets['sst2']['train'].append(sst2_train_label)\n",
    "        dsets['sst2']['test'].append(sst2_test_label)\n",
    "\n",
    "        dsets['imdb']['train'].append(imdb_train_label)\n",
    "        dsets['imdb']['test'].append(imdb_test_label)\n",
    "        \n",
    "\n",
    "    ## split the data based on sample distribution as well as label distribution\n",
    "    sst2_train = concatenate_datasets(dsets=dsets['sst2']['train']).shuffle()\n",
    "    sst2_test = concatenate_datasets(dsets=dsets['sst2']['test']).shuffle()\n",
    "\n",
    "    imdb_train = concatenate_datasets(dsets=dsets['imdb']['train']).shuffle()\n",
    "    imdb_test = concatenate_datasets(dsets=dsets['imdb']['test']).shuffle()\n",
    "\n",
    "\n",
    "    # tokenize the dataset\n",
    "\n",
    "    # this can be done with loop but who cares \n",
    "\n",
    "    # sst2\n",
    "    # train\n",
    "    sst2_train_tokenized = sst2_train.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    sst2_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    # test\n",
    "    sst2_test_tokenized = sst2_test.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    sst2_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "\n",
    "    # imdb\n",
    "    # train\n",
    "    imdb_train_tokenized = imdb_train.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    imdb_train_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "    # test\n",
    "    imdb_test_tokenized = imdb_test.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "    imdb_test_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "\n",
    "    # print(sst2_train_tokenized, sst2_test_tokenized, imdb_train_tokenized, imdb_test_tokenized)\n",
    "\n",
    "    sst2_trainloader = torch.utils.data.DataLoader(dataset = sst2_train_tokenized, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "\n",
    "    sst2_testloader = torch.utils.data.DataLoader(dataset = sst2_test_tokenized, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "    imdb_trainloader = torch.utils.data.DataLoader(dataset = imdb_train_tokenized, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "\n",
    "    imdb_testloader = torch.utils.data.DataLoader(dataset = imdb_test_tokenized, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "\n",
    "    return {\n",
    "        \"sst2\":{\n",
    "            \"train\":sst2_trainloader,\n",
    "            \"valid\":sst2_testloader,\n",
    "            \"test\":sst2_testloader,\n",
    "        },\n",
    "        \"imdb\":{\n",
    "            \"train\":imdb_trainloader,\n",
    "            \"valid\":imdb_testloader,\n",
    "            \"test\":imdb_testloader\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ss_t2 (/home/macab/.cache/huggingface/datasets/ss_t2/default/0.0.0/90167692658fa4abca2ffa3ede1a43a71e2bf671078c5c275c64c4231d5a62fa)\n",
      "Reusing dataset imdb (/home/macab/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n",
      "100%|██████████| 7/7 [00:00<00:00, 35.05ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.82ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 58.81ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 31.54ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 34.59ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.49ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 59.12ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.61ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 35.17ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 37.41ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.67ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 32.24ba/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 34.56ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 36.28ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 31.37ba/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 30.55ba/s]\n",
      "100%|██████████| 7/7 [00:02<00:00,  2.61ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.69ba/s]\n",
      "100%|██████████| 7/7 [00:22<00:00,  3.22s/ba]\n",
      "100%|██████████| 3/3 [00:09<00:00,  3.04s/ba]\n"
     ]
    }
   ],
   "source": [
    "loaders = imdb_sst2_loaders(config=config['tasks']['imdb_sst2_sa'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sst2 828 335 335\nimdb 828 335 335\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders:\n",
    "    print(domain, len(loaders[domain]['train']), len(loaders[domain]['test']), len(loaders[domain]['valid']))"
   ]
  },
  {
   "source": [
    "## 3. MNLI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnli_loaders(config, tokenizer):\n",
    "\n",
    "    domains = config['domains']\n",
    "\n",
    "    train = 'train'\n",
    "    test = 'validation_matched'\n",
    "\n",
    "    # load the dataset\n",
    "    dataset = load_dataset(\"multi_nli\")\n",
    "    dataset = dataset.remove_columns(['pairID', 'promptID', 'premise_binary_parse', 'premise_parse', 'hypothesis_binary_parse', 'hypothesis_parse']) # remove the unrelated fields\n",
    "\n",
    "    labels = set(dataset['train']['label'])\n",
    "\n",
    "    # which label has least number of samples in train data as well as (valid)validation data in all domains\n",
    "    train_label_dist = 25000 # manually checked \n",
    "    test_label_dist = 600 # manually checked \n",
    " \n",
    "\n",
    "    domain_dsets = {}\n",
    "    for domain in domains:\n",
    "\n",
    "        domain_dsets[domain] = {\n",
    "            \"train\":[]\n",
    "        }\n",
    "        domain_dsets[domain].update({\"test\":[]})\n",
    "\n",
    "    \n",
    "    for label in labels:\n",
    "\n",
    "        for domain in domains:\n",
    "\n",
    "            train = dataset['train'].filter(lambda example:example['genre']==domain).filter(lambda example:example['label']==label).select(range(train_label_dist))\n",
    "            test = dataset['validation_matched'].filter(lambda example:example['genre']==domain).filter(lambda example:example['label']==label).select(range(test_label_dist))\n",
    "\n",
    "            domain_dsets[domain]['train'].append(train)\n",
    "            domain_dsets[domain]['test'].append(test)\n",
    "    \n",
    "\n",
    "    # concatenate the dataset and shuffle them\n",
    "    # before it would be list of datasets and after it will be single dataset\n",
    "    for domain in domains:\n",
    "\n",
    "        # concate class label datasets\n",
    "        domain_dsets[domain]['train'] = concatenate_datasets(dsets=domain_dsets[domain]['train']).shuffle()\n",
    "        domain_dsets[domain]['test'] = concatenate_datasets(dsets=domain_dsets[domain]['test']).shuffle()\n",
    "\n",
    "\n",
    "        # # tokenize \n",
    "        domain_dsets[domain]['train'] = domain_dsets[domain]['train'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        domain_dsets[domain]['test'] = domain_dsets[domain]['test'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)\n",
    "        \n",
    "\n",
    "\n",
    "        # change the dtype \n",
    "        domain_dsets[domain]['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "        domain_dsets[domain]['test'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "        # create dataloaders\n",
    "        domain_dsets[domain]['train'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['train'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['test'] = torch.utils.data.DataLoader(dataset = domain_dsets[domain]['test'], batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "        domain_dsets[domain]['valid'] = domain_dsets[domain]['test'] # validation and test will be same\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # why the hell I am doing this?\n",
    "    loaders = domain_dsets\n",
    "    \n",
    "\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0c1db9bb01906573.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-898cee5d2d382de5.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e49de0ed3ac548b0.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-153c9870a40f1da6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e592acff3004bab5.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-23008dd01a782d9c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5dedd4fb23f94a5f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0b183fbad431cd93.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-4d9005413613dfcc.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-50e3f6d87c2d2588.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b655ab8c4b69128f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b2a0e2a28d47007c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-f421b45446c59ac7.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a538299d4fd28437.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5c0024f9bf3852ee.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fd3ed81f369e9c03.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-927d76345e49252b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-cb296e615cf0c952.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-6fa6fae9dcab5274.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-1cd36db539da77d2.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5fac4257da3c539c.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2ce20719b0f395e0.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9166034fb95d402f.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-6ba23de814dc90b4.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-516fde47af368fab.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc45afa6ba2e9bdb.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc8b14e1f98ee9e6.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-17f1d6f29172af7a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e0dc833ee8a2197b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-79960e66c0ba2acd.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-5e04e556de653ba3.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-b3b8ba88c5419e90.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7a569891ee8df246.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-4acd6f099c462a6b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-9a335505dfd25a4b.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-1b5919b2778fe34a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-a286077fe2657368.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2f865b9b2e23546a.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0cca842e0b75ae4d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-aa67b9875687f5f3.arrow\n",
      "Loading cached shuffled indices for dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fe49e24b8c45062d.arrow\n",
      "100%|██████████| 75/75 [00:51<00:00,  1.46ba/s]\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-fe27cbbacb98f6d4.arrow\n",
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.99ba/s]\n",
      "100%|██████████| 75/75 [00:33<00:00,  2.25ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.39ba/s]\n",
      "100%|██████████| 75/75 [00:48<00:00,  1.54ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.67ba/s]\n",
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.75ba/s]\n"
     ]
    }
   ],
   "source": [
    "loaders = mnli_loaders(config=config['tasks'][task], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 75/75 [00:45<00:00,  1.66ba/s]\n"
     ]
    }
   ],
   "source": [
    "data = dsets['slate']['train'].map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True, max_length=config['max_seq_length']), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "government 2344 57 57\ntelephone 2344 57 57\nfiction 2344 57 57\ntravel 2344 57 57\nslate 2344 57 57\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders:\n",
    "    print(domain, len(loaders[domain]['train']), len(loaders[domain]['test']), len(loaders[domain]['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-64e13bf7a635c074.arrow\n",
      "100%|██████████| 78/78 [00:01<00:00, 45.74ba/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['premise', 'hypothesis', 'genre', 'label'],\n",
       "    num_rows: 25783\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "dataset['train'].filter(lambda example:example['genre']==domains[0]).filter(lambda example:example['label']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset multi_nli (/home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"multi_nli\")\n",
    "dataset = dataset.remove_columns(['pairID', 'promptID', 'premise_binary_parse', 'premise_parse', 'hypothesis_binary_parse', 'hypothesis_parse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = config['tasks']['mnli']['domains']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-56053c705d9f0a3d.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-0811af6c0bdbe470.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-e18ac1254bdc5157.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-eb6fe35e67d25fbd.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-d05f2139d738c19a.arrow\n",
      "government [0 1 2] [710 600 635] 1945\n",
      "telephone [0 1 2] [694 619 653] 1966\n",
      "fiction [0 1 2] [695 629 649] 1973\n",
      "travel [0 1 2] [697 624 655] 1976\n",
      "slate [0 1 2] [683 651 621] 1955\n"
     ]
    }
   ],
   "source": [
    "for domain in domains:\n",
    "\n",
    "    lbs, count = np.unique(dataset['validation_matched'].filter(lambda example:example['genre']==domain)['label'], return_counts=True)\n",
    "    print(domain, lbs, count, sum(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-2b4b12d7e08d89e0.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-ffbccc4cc301c667.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-66cee0b143350377.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-7aed209c06bd7807.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-bc42cb155eb5cb84.arrow\n",
      "government [0 1 2] [25783 25784 25783] 77350\n",
      "telephone [0 1 2] [27782 27783 27783] 83348\n",
      "fiction [0 1 2] [25784 25782 25782] 77348\n",
      "travel [0 1 2] [25782 25783 25785] 77350\n",
      "slate [0 1 2] [25768 25768 25770] 77306\n"
     ]
    }
   ],
   "source": [
    "for domain in domains:\n",
    "\n",
    "    lbs, count = np.unique(dataset['train'].filter(lambda example:example['genre']==domain)['label'], return_counts=True)\n",
    "    print(domain, lbs, count, sum(count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = {}\n",
    "\n",
    "for domain in domains:\n",
    "\n",
    "    dsets[domain] = {\n",
    "        \"train\":[]\n",
    "    }\n",
    "    dsets[domain].update({\"test\":[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'government': {'train': [], 'test': []},\n",
       " 'telephone': {'train': [], 'test': []},\n",
       " 'fiction': {'train': [], 'test': []},\n",
       " 'travel': {'train': [], 'test': []},\n",
       " 'slate': {'train': [], 'test': []}}"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = set(dataset['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 <class 'int'>\n1 <class 'int'>\n2 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for each in labels:\n",
    "    print(each, type(each))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset imdb (/home/macab/.cache/huggingface/datasets/imdb/default/0.0.0/959c9f4e491d324dc25551fd2818d074406dee0809641149ea7fcd1c54ecb798)\n"
     ]
    }
   ],
   "source": [
    "dataset_tor = load_dataset(\"toriving/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}