{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0609a2a9752fb5bf4158d35a6e65a5fbe589a915087fd6e60b76f2f7012f50f4b",
   "display_name": "Python 3.8.5 64-bit ('temp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Model\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_book = pd.read_csv(\"/home/macab/research/Adaptil/data/amazon-review/books.csv\")\n",
    "df_book_ul = pd.read_csv(\"/home/macab/research/Adaptil/data/amazon-review/books-UL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     product_type  ... sentiment\n",
       "1982        books  ...         1\n",
       "1983        books  ...         0\n",
       "1984        books  ...         0\n",
       "1985        books  ...         1\n",
       "1986        books  ...         0\n",
       "\n",
       "[5 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_type</th>\n      <th>helpful</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>review_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1982</th>\n      <td>books</td>\n      <td>NaN</td>\n      <td>4.0</td>\n      <td>Check the facts</td>\n      <td>Girl Sleuth is a must-read especially for the ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1983</th>\n      <td>books</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>Why the music?</td>\n      <td>I find some parts of Didion's book very relata...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1984</th>\n      <td>books</td>\n      <td>27 of 33</td>\n      <td>1.0</td>\n      <td>NOT FOR A FAMILY WITH KIDS</td>\n      <td>The photos are beautiful, the designs are just...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1985</th>\n      <td>books</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>A Fantastic Exercise in \"What If\"</td>\n      <td>This isn't really a novel and it isn't really ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1986</th>\n      <td>books</td>\n      <td>3 of 8</td>\n      <td>1.0</td>\n      <td>A major disservice to the book</td>\n      <td>The book THE LOST CONTINENT is an amazing capt...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_book.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  product_type  ...                                        review_text\n",
       "0        books  ...  I'm an employee with a small educational busin...\n",
       "1        books  ...  McCarthy is a very talented writer (read Blood...\n",
       "2        books  ...  Not only is the story heartwarming, the artwor...\n",
       "3        books  ...  Kathy's books are my favorites, and I will alw...\n",
       "4        books  ...  This Dolf de Roos book is no different from hi...\n",
       "\n",
       "[5 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_type</th>\n      <th>helpful</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>review_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>books</td>\n      <td>1 of 1</td>\n      <td>4.0</td>\n      <td>Insightful and thoroughly researched from my p...</td>\n      <td>I'm an employee with a small educational busin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>books</td>\n      <td>3 of 13</td>\n      <td>2.0</td>\n      <td>Melodramatic</td>\n      <td>McCarthy is a very talented writer (read Blood...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>books</td>\n      <td>NaN</td>\n      <td>5.0</td>\n      <td>The Dog Chapel</td>\n      <td>Not only is the story heartwarming, the artwor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>books</td>\n      <td>1 of 1</td>\n      <td>5.0</td>\n      <td>Another one for my collection!</td>\n      <td>Kathy's books are my favorites, and I will alw...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>books</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>The Insider's Guide to Making Money in Real Es...</td>\n      <td>This Dolf de Roos book is no different from hi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df_book_ul.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1987, 6)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_book_ul.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset amazon_reviews_multi/en (download: 82.11 MiB, generated: 58.69 MiB, post-processed: Unknown size, total: 140.79 MiB) to /home/macab/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd...\n",
      "Downloading: 100%|██████████| 2.06M/2.06M [00:06<00:00, 337kB/s]\n",
      "Downloading: 100%|██████████| 2.05M/2.05M [00:08<00:00, 243kB/s]\n",
      "Dataset amazon_reviews_multi downloaded and prepared to /home/macab/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"amazon_reviews_multi\", name=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['train']['product_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN = \"kitchen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 200/200 [00:06<00:00, 33.04ba/s]\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-9d2f6f431303cdfa.arrow\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-308b7c2cdd6eda2a.arrow\n"
     ]
    }
   ],
   "source": [
    "book = data.filter(lambda example: example['product_category']==DOMAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10382"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "len(book['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for each in data['train']['product_category']:\n",
    "    if each == \"movie\":\n",
    "        count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"multi_nli\")\n",
    "# DOMAIN in ['government', 'telephone', 'fiction', 'travel', 'slate'] to train\n",
    "# which user will enter\n",
    "domain_dataset = dataset.filter(lambda example: example['genre'].startswith(DOMAIN))\n",
    "domain_dataset['train'] = domain_dataset['train'].select(range(77306))  # same for training across all domains\n",
    "domain_dataset['validation_matched'] = domain_dataset['validation_matched'].select(range(1945)) # same for validation across all domains\n",
    "encoded_domain_dataset = domain_dataset.map(lambda x: tokenizer(x['premise'], x['hypothesis'], padding='max_length', truncation=True), batched=True)\n",
    "encoded_domain_dataset['train'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "encoded_domain_dataset['validation_matched'].set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset = encoded_domain_dataset['train'], batch_size=32, shuffle=True, num_workers=4)\n",
    "val_data_loader = torch.utils.data.DataLoader(dataset = encoded_domain_dataset['validation_matched'], batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Downloading and preparing dataset csv/default-3c962ed2152802d3 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4...\n",
      "\n",
      "\n",
      "0 tables [00:00, ? tables/s]\u001b[A\u001b[A\n",
      "\n",
      "1 tables [00:00,  7.98 tables/s]\u001b[A\u001b[A\n",
      "\n",
      "                                \u001b[A\u001b[ADataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"csv\", data_files=\"/home/macab/research/Adaptil/data/amazon-review/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1987"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "len(data['train']['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'This is wonderful! It is for fans of Laura Ingalls Wilder, and also for those who are interested in historic cooking - from prairie and colonial days which have their basis in so many of the cultures which settled America in the early days, and adapted for use there'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "data['train']['review_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1987"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(data['train']['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset(\"csv\", data_files=\"/home/macab/research/Adaptil/data/amazon-review/books.csv\", split=\"train[:90%]\")\n",
    "valid = load_dataset(\"csv\", data_files=\"/home/macab/research/Adaptil/data/amazon-review/books.csv\", split=\"train[10%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1788"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "len(train['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', usefast=True, use_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.62s/ba]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:04<00:00,  2.17s/ba]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = train.map(lambda x: tokenizer(x['review_text'], padding='max_length', truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "torch.tensor([encoded_dataset[0]['sentiment']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset = encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "cannot import name 'create_sa_loaders' from 'dataset.dataset' (/home/macab/research/Adaptil/dataset/dataset.py)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-71e327ecd4dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_sa_loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'create_sa_loaders' from 'dataset.dataset' (/home/macab/research/Adaptil/dataset/dataset.py)"
     ]
    }
   ],
   "source": [
    "from dataset.dataset import create_sa_loaders\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', usefast=True, use_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      " 25%|██▌       | 1/4 [00:01<00:05,  1.93s/it]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-b4d4281f510cdb95/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      " 50%|█████     | 2/4 [00:03<00:03,  1.90s/it]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-9c3cc848a6ee4edd/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      " 75%|███████▌  | 3/4 [00:04<00:01,  1.68s/it]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-ffd15131893ced8b/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "loaders = create_sa_loaders(tokenizer=tokenizer, root_dir=\"/home/macab/research/Adaptil/data/amazon-review/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(loaders['books']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = next(iter(loaders['books']['train']))\n",
    "batch_2 = next(iter(loaders['books']['valid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "batch_1['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "batch_2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loaders['books']['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"csv\", data_files=\"/home/macab/research/Adaptil/data/amazon-review/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/macab/research/Adaptil/data/amazon-review/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains =[\"books\", \"dvd\", \"electronics\", \"kitchen_housewares\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?ba/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.50s/ba]\u001b[A\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.43s/ba]\n",
      "  0%|          | 0/4 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2b48e6899532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# create loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# valid_loader = data.DataLoader(dataset = valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "for domain in tqdm(domains):\n",
    "    \n",
    "    d = os.path.join(root_dir, domain+\".csv\")\n",
    "    \n",
    "    # load the data\n",
    "    train = load_dataset(\"csv\", data_files=d)\n",
    "    # valid = load_dataset(\"csv\", data_files=d, split=\"train[75%:]\")\n",
    "    \n",
    "    \n",
    "    # map it with tokenizer \n",
    "    train_dataset = train['train'].map(lambda x: tokenizer(x['review_text'], padding='max_length', truncation=True), batched=True)\n",
    "    # valid_dataset = valid.map(lambda x: tokenizer(x['review_text'], padding='max_length', truncation=True), batched=True)\n",
    "    \n",
    "    \n",
    "    # convert into tensors \n",
    "    train_dataset = train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n",
    "    # valid_dataset = valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n",
    "    \n",
    "    # create loader\n",
    "    train_loader = data.DataLoader(dataset = train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    # valid_loader = data.DataLoader(dataset = valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    \n",
    "    # put the domain laoder into dictionary\n",
    "    loaders[domain] = {\n",
    "        \"train\":train_loader,\n",
    "        # \"valid\":valid_loader\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "Loading cached processed dataset at /home/macab/.cache/huggingface/datasets/csv/default-3c962ed2152802d3/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4/cache-4072fc88f9705a80.arrow\n"
     ]
    }
   ],
   "source": [
    " # load the data\n",
    "train = load_dataset(\"csv\", data_files=d)\n",
    "# valid = load_dataset(\"csv\", data_files=d, split=\"train[75%:]\")\n",
    "\n",
    "\n",
    "# map it with tokenizer \n",
    "train_dataset = train.map(lambda x: tokenizer(x['review_text'], padding='max_length', truncation=True), batched=True)\n",
    "# valid_dataset = valid.map(lambda x: tokenizer(x['review_text'], padding='max_length', truncation=True), batched=True)\n",
    "\n",
    "\n",
    "# convert into tensors \n",
    "train_dataset = train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])\n",
    "# valid_dataset = valid_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({'train': Dataset(features: {'product_type': Value(dtype='string', id=None), 'helpful': Value(dtype='string', id=None), 'rating': Value(dtype='float64', id=None), 'title': Value(dtype='string', id=None), 'review_text': Value(dtype='string', id=None), 'sentiment': Value(dtype='int64', id=None)}, num_rows: 1987)})"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "source": [
    "### Testing the Pipelinne"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer \n",
    "from dataset.dataset import sa_loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base', 'distilroberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "\n",
    "#     config = AutoConfig.from_pretrained(model)\n",
    "\n",
    "#     print(config.hidden_size)\n",
    "#     print(config.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained('distilbert-for-sequence-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-9729425060717359\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-9729425060717359/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset('csv', data_files=\"/home/macab/research/Adaptil/data/amazon-review/books.csv\", split='train[:1582]')\n",
    "        \n",
    "train_dataset = train_dataset.rename_column('sentiment', 'label') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['product_type', 'helpful', 'rating', 'title', 'review_text', 'label'],\n",
       "    num_rows: 1582\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-9729425060717359\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-9729425060717359/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-9729425060717359\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-9729425060717359/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 2/2 [00:12<00:00,  6.39s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00, 17.84ba/s]\n",
      "Using custom data configuration default-4ddecc39120ba9da\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-4ddecc39120ba9da/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-4ddecc39120ba9da\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-4ddecc39120ba9da/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.61ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 19.29ba/s]\n",
      "Using custom data configuration default-bb618980610280a2\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-bb618980610280a2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-bb618980610280a2\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-bb618980610280a2/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.45ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21.27ba/s]\n",
      "Using custom data configuration default-4155c85642e4daa1\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-4155c85642e4daa1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "Using custom data configuration default-4155c85642e4daa1\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-4155c85642e4daa1/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n",
      "100%|██████████| 2/2 [00:00<00:00, 11.43ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.68ba/s]\n"
     ]
    }
   ],
   "source": [
    "loaders = sa_loaders(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loaders['books']['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([16, 128]) torch.Size([16, 128]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "print(batch['input_ids'].shape,  batch['attention_mask'].shape,  batch['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(model_name=models[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(input_ids=batch['input_ids'],  attention_mask=batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Trainer import LightningModel\n",
    "from config import config\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LightningModel(model_name=models[0], task_config=config['tasks']['sa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 110 M \n",
      "Validation sanity check:  50%|█████     | 1/2 [00:00<00:00,  1.75it/s]/home/macab/miniconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 0:   0%|          | 0/396 [00:00<?, ?it/s] /home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  15%|█▍        | 59/396 [01:47<10:12,  1.82s/it, loss=0.633, v_num=3]/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Epoch 0:  15%|█▍        | 59/396 [01:58<11:14,  2.00s/it, loss=0.633, v_num=3]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=lm,\n",
    "    train_dataloader=loaders['books']['valid'],\n",
    "    val_dataloaders=loaders['books']['valid']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.0.3'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-04-24 20:43:30.700468: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-04-24 20:43:30.700497: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/macab/miniconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-2108d0c273ca6677/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "Using custom data configuration default\n",
      "Reusing dataset csv (/home/macab/.cache/huggingface/datasets/csv/default-2108d0c273ca6677/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4)\n",
      "  0%|                                                     | 0/4 [00:05<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 42, in <module>\n",
      "    loaders = create_loaders(task=task, tokenizer=tokenizer)\n",
      "  File \"/home/macab/research/robust-representation-learning/adaptil/dataset/dataset.py\", line 105, in create_loaders\n",
      "    return sa_loaders(tokenizer=tokenizer)\n",
      "  File \"/home/macab/research/robust-representation-learning/adaptil/dataset/dataset.py\", line 38, in sa_loaders\n",
      "    train_dataset = train_dataset.rename_column('sentiment', 'label') \n",
      "AttributeError: 'Dataset' object has no attribute 'rename_column'\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "source": [
    "## Dataset split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data/amazon-review/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{root}*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(50):\n",
    "for file in files:\n",
    "\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "\n",
    "    df_valid_0 = df[df['sentiment']==0].sample(n=150, replace=False, random_state=2)\n",
    "    df = df.drop(index=df_valid_0.index)\n",
    "    df_valid_1 = df[df['sentiment']==1].sample(n=150, replace=False, random_state=2)\n",
    "    df = df.drop(index=df_valid_1.index)\n",
    "\n",
    "\n",
    "    df_valid = pd.concat([df_valid_0, df_valid_1]).sample(frac=1)\n",
    "\n",
    "    # df_valid = df.sample(n=300, replace=False, random_state=2)\n",
    "\n",
    "    # df = df.drop(index=df_valid.index)\n",
    "\n",
    "    domain = file.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    PATH = os.path.join(os.getcwd(), \"data\", \"amazon-review\", domain)\n",
    "    os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "    df_valid.to_csv(os.path.join(PATH, \"valid.csv\"), index=False)\n",
    "    df_valid.to_csv(os.path.join(PATH, \"test.csv\"), index=False)\n",
    "    df.to_csv(os.path.join(PATH, \"train.csv\"), index=False)\n",
    "\n",
    "\n",
    "    # print(df_valid['sentiment'].value_counts())\n",
    "    # print(df_valid.shape[0], df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1774    1\n",
       "1776    1\n",
       "1701    0\n",
       "595     1\n",
       "452     1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "df_valid['sentiment'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  product_type helpful  rating                                          title  \\\n",
       "0  electronics  3 of 4     1.0                                   keep looking   \n",
       "1  electronics  8 of 8     2.0                          Love affair gone sour   \n",
       "2  electronics  2 of 2     5.0                Nothing beats OEM products. . .   \n",
       "3  electronics     NaN     2.0  All setup as described here, UNIT NOT STABLE.   \n",
       "4  electronics  1 of 1     1.0                         Jamming Piece o' Junk!   \n",
       "5  electronics  1 of 6     1.0                              Mouse? What Mouse   \n",
       "6  electronics  1 of 3     5.0                       People complain too much   \n",
       "7  electronics  1 of 2     1.0                                    lousy phone   \n",
       "8  electronics  2 of 2     2.0             Can't find the timer for the radio   \n",
       "9  electronics  2 of 2     5.0                                    The Best!!!   \n",
       "\n",
       "                                         review_text  sentiment  \n",
       "0  I had this 6 months (which is how long it take...          0  \n",
       "1  I have owned this little hub for at least a ye...          0  \n",
       "2  After ordering non-OEM inks from several diffe...          1  \n",
       "3                                                             0  \n",
       "4  I bought this machine based on the good review...          0  \n",
       "5  Amazon does not ship to Post Office Boxes.  Th...          0  \n",
       "6  Hey, don't listen to those other schmucks. So ...          1  \n",
       "7  Surprisingly poor quality for a Uniden phone. ...          0  \n",
       "8  I'd really like to know how I listen to the ra...          0  \n",
       "9  I've owned these Altec's for over three years ...          1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_type</th>\n      <th>helpful</th>\n      <th>rating</th>\n      <th>title</th>\n      <th>review_text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>electronics</td>\n      <td>3 of 4</td>\n      <td>1.0</td>\n      <td>keep looking</td>\n      <td>I had this 6 months (which is how long it take...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>electronics</td>\n      <td>8 of 8</td>\n      <td>2.0</td>\n      <td>Love affair gone sour</td>\n      <td>I have owned this little hub for at least a ye...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>electronics</td>\n      <td>2 of 2</td>\n      <td>5.0</td>\n      <td>Nothing beats OEM products. . .</td>\n      <td>After ordering non-OEM inks from several diffe...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>electronics</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>All setup as described here, UNIT NOT STABLE.</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>electronics</td>\n      <td>1 of 1</td>\n      <td>1.0</td>\n      <td>Jamming Piece o' Junk!</td>\n      <td>I bought this machine based on the good review...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>electronics</td>\n      <td>1 of 6</td>\n      <td>1.0</td>\n      <td>Mouse? What Mouse</td>\n      <td>Amazon does not ship to Post Office Boxes.  Th...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>electronics</td>\n      <td>1 of 3</td>\n      <td>5.0</td>\n      <td>People complain too much</td>\n      <td>Hey, don't listen to those other schmucks. So ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>electronics</td>\n      <td>1 of 2</td>\n      <td>1.0</td>\n      <td>lousy phone</td>\n      <td>Surprisingly poor quality for a Uniden phone. ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>electronics</td>\n      <td>2 of 2</td>\n      <td>2.0</td>\n      <td>Can't find the timer for the radio</td>\n      <td>I'd really like to know how I listen to the ra...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>electronics</td>\n      <td>2 of 2</td>\n      <td>5.0</td>\n      <td>The Best!!!</td>\n      <td>I've owned these Altec's for over three years ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.dataset import create_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/macab/miniconda3/envs/temp/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-c2695105fb2f3ba9\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-c2695105fb2f3ba9/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-c2695105fb2f3ba9/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-d92ab659a85e3384\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-d92ab659a85e3384/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-d92ab659a85e3384/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-d12fd1db019079fd\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-d12fd1db019079fd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-d12fd1db019079fd/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.12s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.48ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.45ba/s]\n",
      "Using custom data configuration default-b7b8d66a0ff7c847\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-b7b8d66a0ff7c847/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-b7b8d66a0ff7c847/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-4462f1e830659ed3\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-4462f1e830659ed3/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-4462f1e830659ed3/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-0d6aa6cbbb431733\n",
      "  0%|          | 0/2 [00:00<?, ?ba/s]Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-0d6aa6cbbb431733/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-0d6aa6cbbb431733/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.15s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.63ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.65ba/s]\n",
      "Using custom data configuration default-73fc9c83fae75a11\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-73fc9c83fae75a11/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-73fc9c83fae75a11/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-c08cc167af4eebf4\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-c08cc167af4eebf4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-c08cc167af4eebf4/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-f6c7cf9d5ee57aeb\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-f6c7cf9d5ee57aeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-f6c7cf9d5ee57aeb/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.13ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.38ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.43ba/s]\n",
      "Using custom data configuration default-6acf065832757345\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-6acf065832757345/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-6acf065832757345/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-19f620757488fae9\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-19f620757488fae9/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-19f620757488fae9/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-46420855dfa912b7\n",
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/macab/.cache/huggingface/datasets/csv/default-46420855dfa912b7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0...\n",
      "Dataset csv downloaded and prepared to /home/macab/.cache/huggingface/datasets/csv/default-46420855dfa912b7/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0. Subsequent calls will reuse this data.\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.23ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  3.09ba/s]\n"
     ]
    }
   ],
   "source": [
    "loaders = create_loaders(task=\"sa\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32, 256])\n",
      "53 10 10\n",
      "torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32, 256])\n",
      "53 10 10\n",
      "torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32, 256])\n",
      "53 10 10\n",
      "torch.Size([32, 256]) torch.Size([32, 256]) torch.Size([32, 256])\n",
      "54 10 10\n"
     ]
    }
   ],
   "source": [
    "for domain in loaders.keys():\n",
    "\n",
    "    train = loaders[domain]['train']\n",
    "    valid = loaders[domain]['valid']\n",
    "    test = loaders[domain]['test']\n",
    "\n",
    "\n",
    "    train_batch = next(iter(train))\n",
    "    valid_batch = next(iter(valid))\n",
    "    test_batch = next(iter(test))\n",
    "\n",
    "    print(train_batch['input_ids'].shape, valid_batch['input_ids'].shape, test_batch['input_ids'].shape)\n",
    "    print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "valid_batch['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}